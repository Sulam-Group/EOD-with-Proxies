{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying Assumption 1 on ACSPublicCoverage Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary packages\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import folktables\n",
    "from folktables import ACSDataSource\n",
    "import matplotlib.pyplot as plt\n",
    "from balancers import BinaryBalancer\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import os\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary functions \n",
    "def public_coverage_filter(data):\n",
    "    \"\"\"\n",
    "    Filters for the public health insurance prediction task; focus on low income Americans, and those not eligible for Medicare\n",
    "    \"\"\"\n",
    "    df = data\n",
    "    df = df[df['AGEP'] < 65]\n",
    "    df = df[df['PINCP'] <= 30000]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set relevant variables\n",
    "results_dir = 'results/'\n",
    "\n",
    "state_list = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI',\n",
    "              'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI',\n",
    "              'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC',\n",
    "              'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT',\n",
    "              'VT', 'VA', 'WA', 'WV', 'WI', 'WY', 'PR']\n",
    "\n",
    "ACStask = folktables.BasicProblem(\n",
    "    features=[\n",
    "        'AGEP',\n",
    "        'SCHL',\n",
    "        'MAR',\n",
    "        'DIS',\n",
    "        'ESP',\n",
    "        'CIT',\n",
    "        'MIG',\n",
    "        'MIL',\n",
    "        'ANC',\n",
    "        'NATIVITY',\n",
    "        'DEAR',\n",
    "        'DEYE',\n",
    "        'DREM',\n",
    "        'PINCP',\n",
    "        'ESR',\n",
    "        'FER',\n",
    "        'RAC1P',\n",
    "    ],\n",
    "    target='PUBCOV',\n",
    "    target_transform=lambda x: x == 1,\n",
    "    group='SEX',\n",
    "    preprocess=public_coverage_filter,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [02:01<00:00,  2.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# Checking which states satisy Assumption 1 (or its TPR/FPR relaxations)\n",
    "assump = []\n",
    "tpr_assump = []\n",
    "fpr_assump = []\n",
    "for state in tqdm(state_list):\n",
    "    # Load state data\n",
    "    data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "    acs_data = data_source.get_data(states =[state], download=True)\n",
    "    x, y, a = ACStask.df_to_numpy(acs_data)\n",
    "    a = (a == 1)*1\n",
    "\n",
    "    # Split data\n",
    "    x_train, x_test, y_train, y_test, a_train, a_test = train_test_split( \n",
    "        x, y, a, test_size=0.25, random_state=0)\n",
    "\n",
    "    # RF classifier for Y\n",
    "    y_model = RandomForestClassifier(max_depth=10)\n",
    "    y_model.fit(x_train, y_train)\n",
    "    y_hat = y_model.predict(x_test)\n",
    "\n",
    "    # RF classifier for A\n",
    "    a_model = RandomForestClassifier(max_depth=10)\n",
    "    a_model.fit(x_train, a_train)\n",
    "    a_hat = a_model.predict(x_test)\n",
    "\n",
    "    # Check assumption\n",
    "    try:\n",
    "        # Create balancer\n",
    "        true_balancer = BinaryBalancer(y=y_test, y_=y_hat, a=a_test, a_hat = a_hat, adjusted = False)\n",
    "        assump.append(true_balancer.assumption)\n",
    "        tpr_assump.append(true_balancer.tpr_assumption)\n",
    "        fpr_assump.append(true_balancer.fpr_assumption)\n",
    "    except:\n",
    "        assump.append('N/A')\n",
    "        tpr_assump.append('N/A')\n",
    "        fpr_assump.append('N/A')\n",
    "df = pd.DataFrame(data=zip(state_list, assump, tpr_assump, fpr_assump),columns=['State', 'Assumption', 'TPR Assumption', 'FPR_Assumption'])\n",
    "df.to_csv(os.path.join(results_dir, 'State_A1Verification.csv'), index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load particular state data\n",
    "state = \"CA\"\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states =[state], download=True)\n",
    "x, y, a = ACStask.df_to_numpy(acs_data)\n",
    "a = (a == 1)*1\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, y_train, y_test, a_train, a_test = train_test_split( \n",
    "    x, y, a, test_size=0.25, random_state=0)\n",
    "\n",
    "# RF classifier for Y\n",
    "y_model = RandomForestClassifier(max_depth=10)\n",
    "y_model.fit(x_train, y_train)\n",
    "y_hat = y_model.predict(x_test)\n",
    "\n",
    "# RF classifier for A\n",
    "a_model = RandomForestClassifier(max_depth=10)\n",
    "a_model.fit(x_train, a_train)\n",
    "a_hat = a_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "# Setting necessary parameters/variables\n",
    "k = 0 \n",
    "iter = 1000\n",
    "y = y_test\n",
    "a = a_test\n",
    "num_rows = len(a)\n",
    "\n",
    "alpha_h_11 = []\n",
    "alpha_h_11_lb = []\n",
    "alpha_h_01 = []\n",
    "alpha_h_01_lb = []\n",
    "alpha_h_10 = []\n",
    "alpha_h_10_lb = []\n",
    "alpha_h_00 = []\n",
    "alpha_h_00_lb = []\n",
    "\n",
    "\n",
    "while k < iter:\n",
    "    random_indices = np.random.choice(num_rows, size=num_rows, replace=True)\n",
    "    a_s = a[random_indices]\n",
    "    y_s = y[random_indices]\n",
    "    a_hat_s = a_hat[random_indices]\n",
    "    y_hat_s = y_hat[random_indices]\n",
    "    true_balancer = BinaryBalancer(y=y_s, y_=y_hat_s, a=a_s, a_hat = a_hat_s, adjusted = False)\n",
    "    \n",
    "    # Calculating bounds of assumption 1 \n",
    "    alpha_h_11.append(true_balancer.a_gr_list[1].tpr)\n",
    "    alpha_h_11_lb.append(true_balancer.U1/true_balancer.est_base_rates['rh_11'])\n",
    "\n",
    "    alpha_h_01.append(true_balancer.a_gr_list[0].tpr)\n",
    "    alpha_h_01_lb.append(true_balancer.U0/true_balancer.est_base_rates['rh_01'])\n",
    "\n",
    "    alpha_h_10.append(true_balancer.a_gr_list[1].fpr)\n",
    "    alpha_h_10_lb.append(true_balancer.U1/true_balancer.est_base_rates['rh_10'])\n",
    "\n",
    "    alpha_h_00.append(true_balancer.a_gr_list[0].fpr)\n",
    "    alpha_h_00_lb.append(true_balancer.U0/true_balancer.est_base_rates['rh_00'])\n",
    "\n",
    "    if k%200 == 0:\n",
    "        print(k)\n",
    "    k = k+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics to show assumption holds\n",
    "rows = ['alpha_h_11', 'alpha_h_11_ub', 'alpha_h_11_lb', \\\n",
    "        'alpha_h_01', 'alpha_h_01_ub', 'alpha_h_01_lb', \\\n",
    "        'alpha_h_10', 'alpha_h_10_ub', 'alpha_h_10_lb', \\\n",
    "        'alpha_h_00', 'alpha_h_00_ub', 'alpha_h_00_lb']\n",
    "values = [np.mean(np.array(alpha_h_11)), np.mean(1 - np.array(alpha_h_11_lb)), np.mean(np.array(alpha_h_11_lb)), \\\n",
    "          np.mean(np.array(alpha_h_01)), np.mean(1 - np.array(alpha_h_01_lb)), np.mean(np.array(alpha_h_01_lb)), \\\n",
    "          np.mean(np.array(alpha_h_10)), np.mean(1 - np.array(alpha_h_10_lb)), np.mean(np.array(alpha_h_10_lb)),\n",
    "          np.mean(np.array(alpha_h_00)), np.mean(1 - np.array(alpha_h_00_lb)), np.mean(np.array(alpha_h_00_lb))]\n",
    "df = pd.DataFrame(data = zip(rows, values), columns = ['Paramater', 'Value'])\n",
    "df.to_csv(os.path.join(results_dir, state + '_assumption_results.csv'), index = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('cuda102_v1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "894a129baa68054ca7e5f212455ea3e06439e084deea6d5def548d13e734b4a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
